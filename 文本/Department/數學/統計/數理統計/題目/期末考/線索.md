---
參考資料:
tags:
---
# 第一大題
幾何分配（失敗次數）
- 問是不是指數族
- 充分統計量定義證明充分
- 請用factorization theorem證明充分
- 完備定義證明統計量完備
- 利用指數族 他的$Q(\theta)$ 包含一個非退化的矩形來證明統計量完備
- 寫出最小充分統計量（因為他是完備充分統計量）或用比值
- 用動差法找出動差估計量
- 找出他的MLE
- 問他的MLE是不是最小充分（MLE如果唯一存在且本身具充分性，則為最小充分）
- 找參數函數的MLE（不變性）
- 把參數空間做限制，試問他的MLE（例如原本範圍是$0<\theta<1\Rightarrow 0.5<\theta<1$）
# 第二大題
已知分配（跟p.294 1.11有點像，$\beta=1、\alpha未知$）
- 證明統計量是不是充分統計量（方式隨意）$T(\underline{X})$
- 證明統計量是不是完備統計量（方式隨意）$T(\underline{X})$
- 證明統計量是輔助統計量$U(\underline{X})$
- 問完備統計量跟輔助統計量有沒有獨立（basu's theorem）
# 第三大題
給一個簡單線性迴歸的模式
- 寫出$\beta_0$、$\beta_1$、$\sigma^2$ 的likelihood function
- 寫出$\beta_0$、$\beta_1$、$\sigma^2$ 的MLE
- 證明$\hat{\beta_0}$是$\beta_0$的不偏估計量
- 證明$\hat{\beta_1}$是$\beta_1$的不偏估計量

# 第一大題
我們直接拿這個來當作我們練習的分配
$$
f(x;\theta)=\theta(1-\theta)^x,\quad x=0,1,2\ldots,\quad \text{where }0<\theta<1
$$
#### 試問此幾何分配是否為指數族？
當我們要判斷一個分配是不是屬於exponential family的時候，我們需要進行以下的步驟：
	1. 寫出分配的pmf/pdf
	2. 把有包含參數跟隨機變數的部分，改寫成exp的形式
	   $$
	   h(x)c(\theta)exp[T(x)Q(\theta)]
	   $$
	   其中：
	   $h(x)$：與$\theta$無關，可以為常數
	   $c(\theta)$：與$\theta$有關，不包含x
	   $T(x)$：與$\theta$無關，須包含x
	   $Q(\theta)$：與$\theta$有關，不可為常數
那我們開始整理
$$
\begin{align}
f(x;\theta)&=\theta(1-\theta)^x\\
&=\theta\cdot exp\lbrace x\ln(1-\theta) \rbrace\\
&=\underbrace{1}_{h(x)}\cdot\underbrace{\theta}_{c(\theta)}\cdot exp\lbrace\underbrace{x}_{T(x)}\cdot\underbrace{\ln(1-\theta)}_{Q(\theta)}\rbrace
\end{align}
$$
故我們可以知道幾何分配為指數族。

考卷我會直接寫：
$$\begin{align}
f(x;\theta)&=\theta(1-\theta)^x\\
&=\theta\cdot exp\lbrace x\ln(1-\theta) \rbrace\\
&=1\cdot\theta\cdot exp\lbrace x\ln(1-\theta)\rbrace\\
\end{align}
$$
$$
\begin{align}
&\because1\cdot\theta\cdot exp\lbrace x\ln(1-\theta)\rbrace\Rightarrow h(x)c(\theta)exp\lbrace T(x)Q(\theta) \rbrace\\
&\text{where }h(x)=1,\, c(\theta)=\theta,\,T(x)=x,\,Q(\theta)=\ln(1-\theta)\\
&\therefore \text{Hence,the Geometric distribution belong to Exponential family}
\end{align}
$$


#### 請用充分統計量的定義證明
我們需要一個統計量來帶我們接下來的題目，我們這邊先用總和$T=\sum^n_{i=1}x_i$做為題目的統計量。

Hence, the Geometric distribution is belong to exponential family


sufficient statistic 充分統計量
A statistic $T(\underline{X})$ is a sufficient statistic for $\theta$ if the conditional distribution of the sample $\underline{X}$ given the value of $T(\underline{X})$ does not depend on $\theta$ that is $f_{\underline{X}\mid T}(\underline{x}\mid t)$ is independent of $\theta$ .

也就是當在給定$T(\underline{X})=t$ 的前提下，若條件分配
$$
f_{\underline{X}\mid T}(\underline{x}\mid t)
=\frac{P(\underline{X}=x,T(\underline{X})=T(x))}{P(T(\underline{X})=t)}
=\frac{P(X=x)}{P(T(\underline{X})=t)}
=\frac{f_{\underline{X}}(x;\theta)}{f_T(t;\theta)}
$$
若此條件分配不含參數$\theta$，則$T(\underline{X})$為充分統計量。
其中
$f_{\underline{X}}(x;\theta)$為joint pmf/pdf。
$f_T(t;\theta)$為T的pmf/pdf。

Since $X_i$ represents the number of failures before the i-th success, the sum $T=\sum^n_{i=1}X_i$ represents the total number of failures before the n-th success,
hence, $T \sim NegBin(n,\theta)$.

$$
\begin{align}
f_{\underline{X}\mid T}(\underline{x}\mid t)=\frac{f_\underline{X}(x;\theta)}{f_T(t;\theta)}&=\frac{\prod^n_{i=1}\theta(1-\theta)^x}{\binom{n+t-1}{t}\theta^n(1-\theta)^t}\quad(Hence,T\sim NegBin(n,\theta))\\
&=\frac{\theta^n(1-\theta)^{\sum^n_{i=1}x_i}}{\binom{n+t-1}{t}\theta^n(1-\theta)^t}\\
&=\frac{\cancel{\theta^n(1-\theta)^{t}}}{\binom{n+t-1}{t}\cancel{\theta^n(1-\theta)^t}}\\
&=\frac{1}{\binom{n+t-1}{t}}
\end{align}
$$
$\because$ $f_{\underline{X}\mid T}(\underline{x}\mid t)$ is independent of parameter $\theta$
$\therefore$ $T(\underline{X})$ is sufficient statistic for $\theta$.

#### 請用factorization theorem證明充分
**Fisher-Neyman Factorization Theorem**
Let $X_1,\ldots,X_n$ be a random sample from $f(x;\theta)$ , $\theta\in \Omega$ , the statistic $T(\underline{X})$ is sufficient for $\theta$ if and only if the joint pdf of $X_1,\ldots,X_n$ factor as follows.
$$
f(\underline{X};\theta)=g(T(\underline{X}),\theta)h(\underline{X})
$$
where g depends on $X_1,\ldots,X_n$ only through T and h is independent of $\theta$

簡單來說，**我們只要把joint pdf整理成$g(T(\underline{X}),\theta)h(\underline{X})$**，就可以直接說by factorization Theorem，$T(\underline{X})$ is sufficient for $\theta$.
其中
$h(\underline{X})$：不含參數$\theta$的任何東西。
$g(T(\underline{X}),\theta)$：含$T(\underline{X})$，可以含$\theta$，可以是常數。

$$
\begin{align}
f(\underline{x};\theta)&=\prod^n_{i=1}\theta(1-\theta)^{x_i}\\
&=\theta^n(1-\theta)^{\sum^n_{i=1}x_i}\\
&=\theta^n(1-\theta)^T
\end{align}
$$
$\because$ where $h(\underline{x})=1$ and $g(T(\underline{X}),\theta)=\theta^n(1-\theta)^T$  。
$\therefore$ By factorization theorem, $T(\underline{X})$ is sufficient statistic.
#### 使用完備定義證明統計量完備
要證明一個分配完備，我們可以找到一個函數$g(x)$，
這個函數會使分配的參數不管為何，他的期望值都是0
也就是$E(g(x))=0$ ,$\forall \theta$。
而我們只要證明只有當g(x)為0函數的情況下，才可能發生上面那個情況。
我們就等於證明了這個分配族完備。

Let g be a function such that $E[g(t)]=0\quad for \,\,0<\theta<1$ 
$$
\begin{align}
E[g(t)]&=\sum^\infty_{t=0}g(t)\binom{n+t-1}{t}\cdot\theta^n(1-\theta)^t\\
&\because \theta^n>0\\
&=\sum^\infty_{t=0}{g(t)\binom{n+t-1}{t}}(1-\theta)^t\\
&\because \sum^\infty_{i=0} a_i\cdot x^i= a_0+a_1\cdot x+a_2\cdot x^2+a_3\cdot x^3+\ldots=0\\
&\Rightarrow a_i=0\quad\text{where }a_i=g(t)\binom{n+t-1}{t}\,and \,x=(1-\theta)\\
&\therefore g(t)\binom{n+t-1}{t}=0\\
&\because \binom{n+t-1}{t}>0\\
&\therefore g(t)=0
\end{align}
$$
Since $E[g(t)]=0$ if and only if $g(t)=0$
Hence, $T(\underline{X})$ is a complete statistic for $\theta$.

#### 利用指數族 他的$Q(\theta)$ 包含一個非退化的矩形來證明統計量完備
$$
\begin{align}
&Q(\theta)=\ln(1-\theta)\\
&\because 0<\theta<1\\
&\therefore \lim_{\theta\rightarrow 0^+}\ln(1-\theta)=0\\
&\quad\lim_{\theta\rightarrow1^-}\ln(1-\theta)=-\infty\\
&\Rightarrow-\infty<Q(\theta)<0\quad(\text{non-degenerate})
\end{align}
$$
It contain a open interval.

Since the Geometric distribution belongs to a one-parameter exponential family, and the natural parameter space is $Q(\theta)\in(-\infty,0)$
Hence, $T(\underline{X})$ is a complete statistic for $\theta$.

#### 證明統計量為最小充分統計量
Since $T(\underline{X})$ is sufficient and complete for $\theta$ ,
and A complete sufficient statistic is always minimal sufficient.
Hence, $T(\underline{X})$ is minimal sufficient statistic.

#### 用動差法找出動差估計量
$$
\begin{align}
\bar{X}&=E[X]\\
\bar{X}&=\frac{1-\theta}{\theta}\\
\bar{X}\theta&=1-\theta\\
\bar{X}\theta+\theta&=1\\
(\bar{X}+1)\theta&=1\\
\hat{\theta}&=\frac{1}{\bar{X}+1}
\end{align}
$$
#### 找到MLE
1. 先算likelihood：
$$
\begin{align}
L(\theta)&=\prod^n_{i=1}\theta(1-\theta)^{x_i}\\
&=\theta^n(1-\theta)^{\sum^n_{i=1}x_i}\\
\end{align}
$$
2. 對likelihood取ln後微分
$$
\begin{align}
\frac{d}{d\theta}\ln L(\theta)&=\frac{d}{d\theta}\ln[\theta^n(1-\theta)^{\sum^n_{i=1} x_i}]\\
&=\frac{d}{d\theta}\left[\ln\theta^n+\ln(1-\theta)^{\sum^n_{i=1}x_i}\right]\\
&=\frac{d}{d\theta}(n\ln\theta)+\frac{d}{d\theta}\left[\sum^n_{i=1}x_i\ln(1-\theta)\right]\\
&=n\frac{1}{\theta}+\sum^n_{i=1}x_i\frac{d}{d\theta}\left[\ln(1-\theta)\right]\\
&=n\frac{1}{\theta}+\sum^n_{i=1}x_i\frac{d}{du}\left[\ln u\right]\frac{du}{d\theta}\\
&=n\frac{1}{\theta}+\sum^n_{i=1}x_i\cdot\frac{1}{u}\frac{du}{d\theta}\\
&=n\frac{1}{\theta}+\sum^n_{i=1}x_i\cdot\frac{1}{u}\cdot(-1)\\
&=\frac{n}{\theta}-\frac{\sum^n_{i=1}x_i}{u}\\
&=\frac{n}{\theta}-\frac{\sum^n_{i=1}x_i}{1-\theta}\\
\end{align}
$$
3. 將其設為0（為了找最大值）解$\theta$
$$
\begin{align}
&\frac{n}{\theta}-\frac{\sum^n_{i=1}x_i}{1-\theta}=0\\
&\frac{n}{\theta}=\frac{\sum^n_{i=1}x_i}{1-\theta}\\
&\theta\cdot\sum^n_{i=1}x_i=n-n\theta\\
&\theta\cdot\left(\sum^n_{i=1}x_i+n\right)=n\\
&\hat{\theta}=\frac{n}{\sum^n_{i=1}x_i+n}\\
&\because\bar{X}=\frac1n\sum^n_{i=1}x_i\Rightarrow\sum^n_{i=1}x_i=n\bar{X}\\
&\therefore\hat{\theta}=\frac{n}{n\bar{X}+n}\\
&\quad\hat{\theta}=\frac{n}{n\bar{X}+n}\\
&\quad\hat{\theta}=\frac{\cancel{n}}{\cancel{n}(\bar{X}+1)}\\
&\quad\hat{\theta}=\frac{1}{\bar{X}+1}
\end{align}
$$
Hence,$\hat{\theta}$ is MLE of $\theta$
#### 問他的MLE是不是最小充分
Since log-likelihood function has unique maximizer, therefore MLE exists uniquly.
and $\hat{\theta}$ 
#### 找參數函數的MLE（不變性）
##### $\hat{\theta}^2$
By invariance property of MLE,
$$
\hat{\theta^2}=\hat{\theta}^2=\left(\frac{1}{\bar{X}+1}\right)^2
$$
##### $\frac{1-\theta}{\theta}$
By invariance property of MLE
$$
\begin{align}
\widehat{\left(\frac{1-\theta}{\theta}\right)}&=\frac{1-\frac{1}{\bar{X}+1}}{\frac{1}{\bar{X}+1}}\\
&=\frac{1}{\frac{1}{\bar{X}+1}}-\frac{\frac{1}{\bar{X}+1}}{\frac{1}{\bar{X}+1}}\\
&=\bar{X}+1-1\\
&=\bar{X}
\end{align}
$$
$\theta(1-\theta)$
By invariance property of MLE
$$
\begin{align}
\hat{[\theta(1-\theta)]}&=[\hat{\theta}(1-\hat{\theta})]\\
&=\frac{1}{\bar{X}+1}\left(1-\frac{1}{\bar{X}+1}\right)\\
&=\frac{1}{\bar{X}+1}-\left(\frac{1}{\bar{X}+1}\right)^2
\end{align}
$$
#### 把參數空間做限制，試問他的MLE（例如原本範圍是$0<\theta<1\Rightarrow 0.5<\theta<1$）
$$
\hat{\theta}=
\begin{cases}
0.5,&\bar{X}>1\\
\frac{1}{\bar{X}+1},&\bar{X}\leq1
\end{cases}
$$
# 第二大題 
假設分配是
$$
f(x;\theta)=e^{-(x-\alpha)}\quad,\text{where }x\geq\alpha,\alpha\in\rm{R},
$$
#### 證明統計量是不是充分統計量（方式隨意）$T(\underline{X})$
就假設$T(\underline{X})$ 是最小值$X_{(n)}$
$$
\begin{align}
f(\underline{X};\alpha)&=\prod^n_{i=1} e^{-(x-\alpha)}I_{[\alpha,\infty)}(x_i)\\
&=e^{n\alpha -\sum x_i}{\prod^n_{i=1}I_{[\alpha,\infty)}(x_i)}\\
&=e^{n\alpha-\sum x_i}I_{[\alpha,\infty)}(x_{(1)})\\\\
&=\underbrace{e^{-\sum x_i}}_{h(\underline{X})}\,\,\underbrace{e^{n\alpha}I_{[\alpha,\infty)}(x_{(1)})}_{g(T(\underline{X}),\alpha)}
\end{align}
$$
By factorization Theorem, $T(\underline{X})=X_{(1)}$ is sufficient statistic for $\alpha$ .
#### 證明統計量是不是完備統計量（方式隨意）$T(\underline{X})$
$$

$$
#### 證明統計量是輔助統計量$U(\underline{X})$
假設統計量是$x_i-x_{(1)}$
let $y_i=x_i-\alpha\Rightarrow x_i=y_i+\alpha$ 
$\because$ $e^{-y_i}$
$\therefore y_i\sim Exp(1)$ 
we can write the statistic as  $y_i+\alpha-y_{(1)}-\alpha\Rightarrow y_i-y_{(1)}$
$\because$ the distribution of statistic does not depend on $\alpha$
$\therefore$ the statistic $x_i-x_{(1)}$ is Ancillary statistic for $\alpha$.

#### 問完備統計量跟輔助統計量有沒有獨立（basu's theorem）
Since, $T(\underline{X})$ is complete sufficient statistic for $\alpha$, and $U(\underline{X})$ is Ancillary statistic for $\alpha$
Hence, by Basu's Theorem $T(\underline{X})$ is independent of $U(\underline{X})$ 
# 第三大題
假設題目給定的簡單線性迴歸為
$$
y_i=\beta_0+\beta_1x+\epsilon_i
$$
#### 寫出$\beta_0$、$\beta_1$、$\sigma^2$ 的likelihood function
under $\epsilon\sim N(0,\sigma^2)\Rightarrow y_i\sim N(\beta_0+\beta_1x_i,\sigma^2)$
$$
\begin{align}
L(\beta_0,\beta_1,\sigma^2)&=\prod^n_{i=1}\frac{1}{\sqrt{2\pi\sigma^2}}exp\left\lbrace-\frac{1}{2\sigma^2}(y_i-\beta_0-\beta_1x_i)^2\right\rbrace\\
&=\frac{1}{(\sqrt{2\pi\sigma^2})^n}exp\left\lbrace-\frac{1}{2\sigma^2}\sum^n_{i=1}(y_i-\beta_0-\beta_1x_i)^2\right\rbrace
\end{align}
$$

#### 寫出$\beta_0$、$\beta_1$、$\sigma^2$ 的MLE
先取log
$$
\begin{align}
\ln L(\beta_0,\beta_1,\sigma^2)&=\ln\left\lbrace\frac{1}{(\sqrt{2\pi\sigma^2})^n}exp\left\lbrace-\frac{1}{2\sigma^2}\sum^n_{i=1}(y_i-\beta_0-\beta_1x_i)^2\right\rbrace\right\rbrace\\
&=\ln\left\lbrace\frac{1}{(\sqrt{2\pi\sigma^2})^n}\right\rbrace+\ln\left\lbrace exp\left\lbrace-\frac{1}{2\sigma^2}\sum^n_{i=1}(y_i-\beta_0-\beta_1x_i)^2\right\rbrace\right\rbrace\\
&=\ln\left\lbrace\frac{1}{(\sqrt{2\pi\sigma^2})^n}\right\rbrace-\frac{1}{2\sigma^2}\sum^n_{i=1}(y_i-\beta_0-\beta_1x_i)^2\\
&=\ln\left\lbrace\frac{1}{(2\pi\sigma^2)^{n/2}}\right\rbrace-\frac{1}{2\sigma^2}\sum^n_{i=1}(y_i-\beta_0-\beta_1x_i)^2\\
&=\ln\left\lbrace(2\pi\sigma^2)^{-n/2}\right\rbrace-\frac{1}{2\sigma^2}\sum^n_{i=1}(y_i-\beta_0-\beta_1x_i)^2\\
&=\ln(2^{-n/2}\pi^{-n/2}\sigma^{2\cdot(-n/2)})-\frac{1}{2\sigma^2}\sum^n_{i=1}(y_i-\beta_0-\beta_1x_i)^2\\
&=\ln2\pi^{-n/2}+\ln\sigma^{2\cdot(-n/2)}-\frac{1}{2\sigma^2}\sum^n_{i=1}(y_i-\beta_0-\beta_1x_i)^2\\
&=\frac{-n}{2}\ln2\pi-\frac{n}{2}\ln\sigma^2-\frac{1}{2\sigma^2}\sum^n_{i=1}(y_i-\beta_0-\beta_1x_i)^2\\
\end{align}
$$
先算$\beta_0$的MLE
微分
$$
\begin{align}
\frac{d}{d\beta_0}\ln L(\beta_0,\beta_1,\sigma^2)&=\frac{d}{d\beta_0}\left\lbrace\frac{-n}{2}\ln2\pi-\frac{n}{2}\ln\sigma^2-\frac{1}{2\sigma^2}\sum^n_{i=1}(y_i-\beta_0-\beta_1x_i)^2\right\rbrace\\
&=\frac{d}{d\beta_0}\left\lbrace-\frac{1}{2\sigma^2}\sum^n_{i=1}(y_i-\beta_0-\beta_1x_i)^2\right\rbrace\\
&=\sum^n_{i=1}\frac{d}{du}\left\lbrace-\frac{1}{2\sigma^2}u^2\cdot\frac{du}{d\beta_0}\right\rbrace\\
&=\sum^n_{i=1}\frac{d}{du}\left\lbrace-\frac{1}{2\sigma^2}2u\cdot\frac{d\beta_0}{du}\right\rbrace\\
&=\sum^n_{i=1}\left\lbrace-\frac{1}{2\sigma^2}2u\cdot(-1)\right\rbrace\\
&=\sum^n_{i=1}\left\lbrace\frac{1}{\sigma^2}u\right\rbrace\\
&=\sum^n_{i=1}\left\lbrace\frac{y_i-\beta_0-\beta_1x_i}{\sigma^2}\right\rbrace\\
&=\sum^n_{i=1}\left( y_i-\beta_0-\beta_1x_i\right)\\
\end{align}
$$
取0
$$
\begin{align}
&\sum^n_{i=1}\left( y_i-\beta_0-\beta_1x_i\right)=0\\
&\sum^n_{i=1}y_i-n\beta_0-\beta_1\sum^n_{i=1}x_i=0\\
&n\beta_0=\sum^n_{i=1}y_i-\beta_1\sum^n_{i=1}x_i\\
&\beta_0=\frac{\sum^n_{i=1}y_i}{n}-\beta_1\frac{\sum^n_{i=1}x_i}{n}\\
&\hat{\beta_0}=\bar{y}-\beta_1\bar{x}
\end{align}
$$
算$\beta_1$的MLE
微分
$$
\begin{align}
\frac{d}{d\beta_0}\ln L(\beta_0,\beta_1,\sigma^2)&=\frac{d}{d\beta_1}\left\lbrace\frac{-n}{2}\ln2\pi-\frac{n}{2}\ln\sigma^2-\frac{1}{2\sigma^2}\sum^n_{i=1}(y_i-\beta_0-\beta_1x_i)^2\right\rbrace\\
&=\frac{d}{d\beta_1}\left\lbrace-\frac{1}{2\sigma^2}\sum^n_{i=1}(y_i-\beta_0-\beta_1x_i)^2\right\rbrace\\
&=-\frac{1}{2\sigma^2}\sum^n_{i=1}\frac{d}{d\beta_1}\left\lbrace(y_i-\beta_0-\beta_1x_i)^2\right\rbrace\\
&=-\frac{1}{2\sigma^2}\sum^n_{i=1}\frac{d}{du}\left\lbrace u^2\frac{du}{d\beta_1}\right\rbrace\\
&=-\frac{1}{2\sigma^2}\sum^n_{i=1}\left\lbrace 2u\cdot(-x_i)\right\rbrace\\
&=-\frac{1}{2\sigma^2}\sum^n_{i=1}\left\lbrace -2x_i(y_i-\beta_0-\beta_1x_i)\right\rbrace\\
&=\sum^n_{i=1}\left\lbrace -\frac{-2x_i(y_i-\beta_0-\beta_1x_i)}{2\sigma^2}\right\rbrace\\
&=\sum^n_{i=1}\left\lbrace \frac{x_i(y_i-\beta_0-\beta_1x_i)}{\sigma^2}\right\rbrace\\
\end{align}
$$
取0
$$
\begin{align}
&\sum^n_{i=1}\left\lbrace \frac{x_i(y_i-\beta_0-\beta_1x_i)}{\sigma^2}\right\rbrace=0\\
&\frac{1}{\sigma^2}\sum^n_{i=1}x_i(y_i-\beta_0-\beta_1x_i)=0\\
&\sum^n_{i=1}x_i(y_i-\beta_0-\beta_1x_i)=0\\
&\sum^n_{i=1}x_i\cdot y_i-\sum^n_{i=1}\beta_0x_i-\beta_1\sum^n_{i=1}x_i^2=0\\
&\sum^n_{i=1}x_i\cdot y_i-\sum^n_{i=1}\beta_0x_i-\beta_1\sum^n_{i=1}x_i^2=0\\
&\hat{\beta_1}=\frac{\sum^n_{i=1}x_i\cdot y_i-\sum^n_{i=1}\beta_0x_i}{\sum^n_{i=1}x_i^2}\\
&\hat{\beta_1}=\frac{\sum^n_{i=1}x_i\cdot y_i-\sum^n_{i=1}(\bar{y}-\beta_1\bar{x})x_i}{\sum^n_{i=1}x_i^2}\\
&\hat{\beta_1}=\frac{\sum^n_{i=1}x_i\cdot y_i-\sum^n_{i=1}\bar{y}\cdot x_i-\sum^n_{i=1}\beta_1\bar{x}\cdot x_i}{\sum^n_{i=1}x_i^2}\\
&\hat{\beta_1}=\frac{\sum^n_{i=1}x_i\cdot y_i-\bar{y}\sum^n_{i=1}x_i+\beta_1\bar{x}\sum^n_{i=1} x_i}{\sum^n_{i=1}x_i^2}\\
&\hat{\beta_1}\cdot\sum^n_{i=1}x_i^2=\sum^n_{i=1}x_i\cdot y_i-\bar{y}\sum^n_{i=1}x_i+\beta_1\bar{x}\sum^n_{i=1} x_i\\
&\hat{\beta_1}\cdot\sum^n_{i=1}x_i^2-\beta_1\bar{x}\sum^n_{i=1} x_i=\sum^n_{i=1}x_i\cdot y_i-\bar{y}\sum^n_{i=1}x_i\\
&\hat{\beta_1}=\frac{\sum^n_{i=1}x_i\cdot y_i-\bar{y}\sum^n_{i=1}x_i}{\sum^n_{i=1}x_i^2-\bar{x}\sum^n_{i=1} x_i}\\
&\hat{\beta_1}=\frac{\sum^n_{i=1}(x_i-\bar{x})(y_i-\bar{y})}{\sum^n_{i=1}(x_i-\bar{x})^2}\\
\end{align}
$$

算$\sigma^2$的MLE
微分
$$
\begin{align}
\frac{d}{d\sigma^2}\ln L(\beta_0,\beta_1,\sigma^2)&=\frac{d}{d\sigma^2}\left\lbrace\frac{-n}{2}\ln2\pi-\frac{n}{2}\ln\sigma^2-\frac{1}{2\sigma^2}\sum^n_{i=1}(y_i-\beta_0-\beta_1x_i)^2\right\rbrace\\
&=\frac{d}{d\sigma^2}\left\lbrace-\frac{n}{2}\ln\sigma^2-\frac{1}{2\sigma^2}\sum^n_{i=1}(y_i-\beta_0-\beta_1x_i)^2\right\rbrace\\
&=-\frac{n}{2\sigma^2}+\sum^n_{i=1}(y_i-\beta_0-\beta_1x_i)^2\frac{d}{d\sigma^2}\left\lbrace-\frac{1}{2\sigma^2}\right\rbrace\\
&=-\frac{n}{2\sigma^2}-\frac{\sum^n_{i=1}(y_i-\beta_0-\beta_1x_i)^2}{2(\sigma^2)^2}\\
\end{align}
$$
取0
$$
\begin{align}
&-\frac{n}{2\sigma^2}-\frac{\sum^n_{i=1}(y_i-\beta_0-\beta_1x_i)^2}{2(\sigma^2)^2}=0\\
&-\frac{n}{2\sigma^2}=\frac{\sum^n_{i=1}(y_i-\beta_0-\beta_1x_i)^2}{2(\sigma^2)^2}\\
&\sigma^2=\frac{\sum^n_{i=1}(y_i-\beta_0-\beta_1x_i)^2}{n}\\
\end{align}
$$


#### 證明$\hat{\beta_0}$是$\beta_0$的不偏估計量
#### 證明$\hat{\beta_1}$是$\beta_1$的不偏估計量
